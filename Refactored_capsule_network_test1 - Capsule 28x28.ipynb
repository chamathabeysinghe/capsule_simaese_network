{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from matplotlib.pyplot import imread\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng\n",
    "from keras.utils import plot_model\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "from time import time\n",
    "import glob\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please give below paths as per the paths in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"data/images_background/\"\n",
    "val_folder = 'data/images_evaluation/'\n",
    "save_path = 'data/'\n",
    "train_type = 'convolutional' #CONVOLUTIONAL or CAPSULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs(path,n = 0):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    # we load every alphabet seperately so we can isolate them later\n",
    "    for alphabet in [x for x in os.listdir(path) if not x.startswith('.')]:\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        # every letter/category has it's own column in the array, so  load seperately\n",
    "        for letter in [x for x in os.listdir(alphabet_path) if not x.startswith('.')]:\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            # read all the images in the current category\n",
    "            for filename in [x for x in os.listdir(letter_path) if not x.startswith('.')]:\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "#                 image = resize(image, (28, 28), anti_aliasing=True)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X,y,lang_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Arcadian\n",
      "loading alphabet: Greek\n",
      "loading alphabet: Armenian\n",
      "loading alphabet: Latin\n",
      "loading alphabet: Futurama\n",
      "loading alphabet: N_Ko\n",
      "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Alphabet_of_the_Magi\n",
      "loading alphabet: Burmese_(Myanmar)\n",
      "loading alphabet: Hebrew\n",
      "loading alphabet: Syriac_(Estrangelo)\n",
      "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Early_Aramaic\n",
      "loading alphabet: Braille\n",
      "loading alphabet: Asomtavruli_(Georgian)\n",
      "loading alphabet: Mkhedruli_(Georgian)\n",
      "loading alphabet: Tagalog\n",
      "loading alphabet: Balinese\n",
      "loading alphabet: Japanese_(katakana)\n",
      "loading alphabet: Malay_(Jawi_-_Arabic)\n",
      "loading alphabet: Tifinagh\n",
      "loading alphabet: Anglo-Saxon_Futhorc\n",
      "loading alphabet: Korean\n",
      "loading alphabet: Grantha\n",
      "loading alphabet: Bengali\n",
      "loading alphabet: Gujarati\n",
      "loading alphabet: Japanese_(hiragana)\n",
      "loading alphabet: Sanskrit\n",
      "loading alphabet: Cyrillic\n",
      "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Aurek-Besh\n",
      "loading alphabet: Atemayar_Qelisayer\n",
      "loading alphabet: Malayalam\n",
      "loading alphabet: Angelic\n",
      "loading alphabet: Kannada\n",
      "loading alphabet: Oriya\n",
      "loading alphabet: Keble\n",
      "loading alphabet: ULOG\n",
      "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
      "loading alphabet: Syriac_(Serto)\n",
      "loading alphabet: Sylheti\n",
      "loading alphabet: Tibetan\n",
      "loading alphabet: Tengwar\n",
      "loading alphabet: Avesta\n",
      "loading alphabet: Mongolian\n",
      "loading alphabet: Gurmukhi\n",
      "loading alphabet: Manipuri\n",
      "loading alphabet: Glagolitic\n",
      "loading alphabet: Ge_ez\n",
      "loading alphabet: Atlantean\n"
     ]
    }
   ],
   "source": [
    "X,y,c=loadimgs(train_folder)\n",
    "Xval,yval,cval=loadimgs(val_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the tensors on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path,\"train.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((X,c),f)\n",
    "\n",
    "with open(os.path.join(save_path,\"val.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((Xval,cval),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, name=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, name=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training alphabets: \n",
      "\n",
      "['N_Ko', 'Japanese_(katakana)', 'Early_Aramaic', 'Tifinagh', 'Burmese_(Myanmar)', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Greek', 'Futurama', 'Hebrew', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Sanskrit', 'Cyrillic', 'Bengali', 'Arcadian', 'Latin', 'Alphabet_of_the_Magi', 'Syriac_(Estrangelo)', 'Balinese', 'Armenian', 'Malay_(Jawi_-_Arabic)', 'Asomtavruli_(Georgian)', 'Gujarati', 'Korean', 'Braille', 'Tagalog', 'Anglo-Saxon_Futhorc', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Mkhedruli_(Georgian)', 'Grantha', 'Japanese_(hiragana)']\n",
      "Validation alphabets:\n",
      "\n",
      "['Old_Church_Slavonic_(Cyrillic)', 'ULOG', 'Atemayar_Qelisayer', 'Gurmukhi', 'Keble', 'Sylheti', 'Syriac_(Serto)', 'Aurek-Besh', 'Manipuri', 'Glagolitic', 'Avesta', 'Ge_ez', 'Angelic', 'Tibetan', 'Tengwar', 'Oriya', 'Atlantean', 'Mongolian', 'Malayalam', 'Kannada']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_path, \"train.pickle\"), \"rb\") as f:\n",
    "    (Xtrain, train_classes) = pickle.load(f)\n",
    "    \n",
    "print(\"Training alphabets: \\n\")\n",
    "print(list(train_classes.keys()))\n",
    "\n",
    "with open(os.path.join(save_path, \"val.pickle\"), \"rb\") as f:\n",
    "    (Xval, val_classes) = pickle.load(f)\n",
    "\n",
    "print(\"Validation alphabets:\", end=\"\\n\\n\")\n",
    "print(list(val_classes.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,s=\"train\"):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "    \n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    # initialize vector for the targets\n",
    "    targets=np.zeros((batch_size,))\n",
    "    \n",
    "    # make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = rng.randint(0, n_examples)\n",
    "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        idx_2 = rng.randint(0, n_examples)\n",
    "        \n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            # add a random number to the category modulo n classes to ensure 2nd image has a different category\n",
    "            category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "        \n",
    "        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "    \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size,s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\", language=None):\n",
    "    \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    indices = rng.randint(0, n_examples,size=(N,))\n",
    "    if language is not None: # if language is specified, select characters for that language\n",
    "        low, high = categories[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "\n",
    "    else: # if no language specified just pick a bunch of random letters\n",
    "        categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "    test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
    "    support_set = X[categories,indices,:,:]\n",
    "    support_set[0,:,:] = X[true_category,ex2]\n",
    "    support_set = support_set.reshape(N, w, h,1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image,support_set]\n",
    "\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capsnet_model(input_shape, n_class, routings):\n",
    "\n",
    "    left_input = Input(shape=input_shape)\n",
    "    right_input = Input(shape=input_shape)\n",
    "\n",
    "    input = Input(shape=input_shape)\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1',\n",
    "                  kernel_initializer=initialize_weights, bias_initializer=initialize_bias,\n",
    "                   kernel_regularizer=l2(2e-4),)(input)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
    "    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
    "                             name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='capsnet')(digitcaps)\n",
    "\n",
    "    tunnel = Model(input, out_caps)\n",
    "    tunnel.summary()\n",
    "\n",
    "    encoded_l = tunnel(left_input)\n",
    "    encoded_r = tunnel(right_input)\n",
    "\n",
    "    L1_layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    prediction = Dense(1, activation='sigmoid', bias_initializer=initialize_bias)(L1_distance)\n",
    "\n",
    "    train_model = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    return train_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 2 # interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "n_iter = 20000 # No. of training iterations\n",
    "N_way = 20 # how many classes for testing one-shot tasks\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "best = -1\n",
    "\n",
    "model_path = 'weights/cnn_siamese'\n",
    "\n",
    "os.makedirs(\"checkpoints_{0}\".format(train_type), exist_ok=True)\n",
    "file_path = \"checkpoints_{0}\".format(train_type)+\"/siamese-epoch-{epoch:05d}-lr-\" + \"-train_loss-{loss:.4f}-val_loss-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path,\n",
    "                             monitor=['loss', 'val_loss'],\n",
    "                             verbose=1,\n",
    "                             save_best_only=False,\n",
    "                             save_weights_only=False,\n",
    "                             mode='min',\n",
    "                             period=2)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()), histogram_freq=0)\n",
    "lr_decay = LearningRateScheduler(schedule=lambda epoch: 0.001 * (0.9 ** epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***New model for convolutional network loaded***\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 4096)         38947648    input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 4096)         0           sequential_7[1][0]               \n",
      "                                                                 sequential_7[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            4097        lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,951,745\n",
      "Trainable params: 38,951,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ckpts = glob.glob(\"checkpoints_{0}\".format(train_type)+\"/*.hdf5\")\n",
    "initial_epoch = 0\n",
    "if len(ckpts) != 0 and False:\n",
    "    latest_ckpt = max(ckpts, key=os.path.getctime)\n",
    "    print(\"loading from checkpoint: \", latest_ckpt)\n",
    "    initial_epoch = int(latest_ckpt[latest_ckpt.find(\"-epoch-\") + len(\"-epoch-\"):latest_ckpt.rfind(\"-lr-\")])\n",
    "    model = load_model(latest_ckpt, custom_objects={'CapsuleLayer': CapsuleLayer, 'Length': Length, 'PrimaryCap':PrimaryCap })\n",
    "else:\n",
    "    if train_type == 'capsule':\n",
    "        print(\"***New model for capsule network loaded***\")\n",
    "        model = get_capsnet_model(input_shape=[28, 28, 1], n_class=10, routings=3)\n",
    "    elif train_type == 'convolutional':\n",
    "        print(\"***New model for convolutional network loaded***\")\n",
    "        model = get_siamese_model((105, 105, 1))\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png')\n",
    "Image(retina=True, filename='model.png')\n",
    "\n",
    "optimizer = Adam(lr = 0.001)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 1.3623 - val_loss: 0.9378\n",
      "Epoch 2/1000\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.9669 - val_loss: 0.9894\n",
      "\n",
      "Epoch 00002: saving model to checkpoints_convolutional/siamese-epoch-00002-lr--train_loss-0.9669-val_loss-0.9894.hdf5\n",
      "Epoch 3/1000\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 1.0050 - val_loss: 1.0037\n",
      "Epoch 4/1000\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.9991 - val_loss: 0.9933\n",
      "\n",
      "Epoch 00004: saving model to checkpoints_convolutional/siamese-epoch-00004-lr--train_loss-0.9991-val_loss-0.9933.hdf5\n",
      "Epoch 5/1000\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.9877 - val_loss: 0.9834\n",
      "Epoch 6/1000\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.9792 - val_loss: 0.9766\n",
      "\n",
      "Epoch 00006: saving model to checkpoints_convolutional/siamese-epoch-00006-lr--train_loss-0.9792-val_loss-0.9766.hdf5\n",
      "Epoch 7/1000\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.9754 - val_loss: 0.9743\n",
      "Epoch 8/1000\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.9711 - val_loss: 0.9696\n",
      "\n",
      "Epoch 00008: saving model to checkpoints_convolutional/siamese-epoch-00008-lr--train_loss-0.9711-val_loss-0.9696.hdf5\n",
      "Epoch 9/1000\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.9680 - val_loss: 0.9644\n",
      "Epoch 10/1000\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 0.9622 - val_loss: 0.9588\n",
      "\n",
      "Epoch 00010: saving model to checkpoints_convolutional/siamese-epoch-00010-lr--train_loss-0.9622-val_loss-0.9588.hdf5\n",
      "Epoch 11/1000\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 0.9567 - val_loss: 0.9542\n",
      "Epoch 12/1000\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.9525 - val_loss: 0.9506\n",
      "\n",
      "Epoch 00012: saving model to checkpoints_convolutional/siamese-epoch-00012-lr--train_loss-0.9525-val_loss-0.9506.hdf5\n",
      "Epoch 13/1000\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.9482 - val_loss: 0.9459\n",
      "Epoch 14/1000\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.9439 - val_loss: 0.9422\n",
      "\n",
      "Epoch 00014: saving model to checkpoints_convolutional/siamese-epoch-00014-lr--train_loss-0.9439-val_loss-0.9422.hdf5\n",
      "Epoch 15/1000\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.9399 - val_loss: 0.9378\n",
      "Epoch 16/1000\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.9358 - val_loss: 0.9338\n",
      "\n",
      "Epoch 00016: saving model to checkpoints_convolutional/siamese-epoch-00016-lr--train_loss-0.9358-val_loss-0.9338.hdf5\n",
      "Epoch 17/1000\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.9320 - val_loss: 0.9302\n",
      "Epoch 18/1000\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.9285 - val_loss: 0.9268\n",
      "\n",
      "Epoch 00018: saving model to checkpoints_convolutional/siamese-epoch-00018-lr--train_loss-0.9285-val_loss-0.9268.hdf5\n",
      "Epoch 19/1000\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.9254 - val_loss: 0.9239\n",
      "Epoch 20/1000\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.9226 - val_loss: 0.9207\n",
      "\n",
      "Epoch 00020: saving model to checkpoints_convolutional/siamese-epoch-00020-lr--train_loss-0.9226-val_loss-0.9207.hdf5\n",
      "Epoch 21/1000\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.9201 - val_loss: 0.9188\n",
      "Epoch 22/1000\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.9177 - val_loss: 0.9166\n",
      "\n",
      "Epoch 00022: saving model to checkpoints_convolutional/siamese-epoch-00022-lr--train_loss-0.9177-val_loss-0.9166.hdf5\n",
      "Epoch 23/1000\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.9155 - val_loss: 0.9147\n",
      "Epoch 24/1000\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.9135 - val_loss: 0.9125\n",
      "\n",
      "Epoch 00024: saving model to checkpoints_convolutional/siamese-epoch-00024-lr--train_loss-0.9135-val_loss-0.9125.hdf5\n",
      "Epoch 25/1000\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.9115 - val_loss: 0.9107\n",
      "Epoch 26/1000\n",
      "100/100 [==============================] - 5s 45ms/step - loss: 0.9099 - val_loss: 0.9090\n",
      "\n",
      "Epoch 00026: saving model to checkpoints_convolutional/siamese-epoch-00026-lr--train_loss-0.9099-val_loss-0.9090.hdf5\n",
      "Epoch 27/1000\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.9082 - val_loss: 0.9074\n",
      "Epoch 28/1000\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.9067 - val_loss: 0.9060\n",
      "\n",
      "Epoch 00028: saving model to checkpoints_convolutional/siamese-epoch-00028-lr--train_loss-0.9067-val_loss-0.9060.hdf5\n",
      "Epoch 29/1000\n",
      "100/100 [==============================] - 5s 48ms/step - loss: 0.9054 - val_loss: 0.9047\n",
      "Epoch 30/1000\n",
      "100/100 [==============================] - 5s 50ms/step - loss: 0.9041 - val_loss: 0.9035\n",
      "\n",
      "Epoch 00030: saving model to checkpoints_convolutional/siamese-epoch-00030-lr--train_loss-0.9041-val_loss-0.9035.hdf5\n",
      "Epoch 31/1000\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 0.9029 - val_loss: 0.9024\n",
      "Epoch 32/1000\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.9017 - val_loss: 0.9013\n",
      "\n",
      "Epoch 00032: saving model to checkpoints_convolutional/siamese-epoch-00032-lr--train_loss-0.9017-val_loss-0.9013.hdf5\n",
      "Epoch 33/1000\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 0.9008 - val_loss: 0.9004\n",
      "Epoch 34/1000\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.8998 - val_loss: 0.8996\n",
      "\n",
      "Epoch 00034: saving model to checkpoints_convolutional/siamese-epoch-00034-lr--train_loss-0.8998-val_loss-0.8996.hdf5\n",
      "Epoch 35/1000\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.8989 - val_loss: 0.8985\n",
      "Epoch 36/1000\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8981 - val_loss: 0.8976\n",
      "\n",
      "Epoch 00036: saving model to checkpoints_convolutional/siamese-epoch-00036-lr--train_loss-0.8981-val_loss-0.8976.hdf5\n",
      "Epoch 37/1000\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.8973 - val_loss: 0.8970\n",
      "Epoch 38/1000\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.8966 - val_loss: 0.8961\n",
      "\n",
      "Epoch 00038: saving model to checkpoints_convolutional/siamese-epoch-00038-lr--train_loss-0.8966-val_loss-0.8961.hdf5\n",
      "Epoch 39/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8959 - val_loss: 0.8956\n",
      "Epoch 40/1000\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.8952 - val_loss: 0.8950\n",
      "\n",
      "Epoch 00040: saving model to checkpoints_convolutional/siamese-epoch-00040-lr--train_loss-0.8952-val_loss-0.8950.hdf5\n",
      "Epoch 41/1000\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.8947 - val_loss: 0.8944\n",
      "Epoch 42/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8941 - val_loss: 0.8939\n",
      "\n",
      "Epoch 00042: saving model to checkpoints_convolutional/siamese-epoch-00042-lr--train_loss-0.8941-val_loss-0.8939.hdf5\n",
      "Epoch 43/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8936 - val_loss: 0.8934\n",
      "Epoch 44/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8931 - val_loss: 0.8929\n",
      "\n",
      "Epoch 00044: saving model to checkpoints_convolutional/siamese-epoch-00044-lr--train_loss-0.8931-val_loss-0.8929.hdf5\n",
      "Epoch 45/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8926 - val_loss: 0.8926\n",
      "Epoch 46/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8922 - val_loss: 0.8920\n",
      "\n",
      "Epoch 00046: saving model to checkpoints_convolutional/siamese-epoch-00046-lr--train_loss-0.8922-val_loss-0.8920.hdf5\n",
      "Epoch 47/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8917 - val_loss: 0.8916\n",
      "Epoch 48/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8914 - val_loss: 0.8912\n",
      "\n",
      "Epoch 00048: saving model to checkpoints_convolutional/siamese-epoch-00048-lr--train_loss-0.8914-val_loss-0.8912.hdf5\n",
      "Epoch 49/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8910 - val_loss: 0.8905\n",
      "Epoch 50/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8907 - val_loss: 0.8908\n",
      "\n",
      "Epoch 00050: saving model to checkpoints_convolutional/siamese-epoch-00050-lr--train_loss-0.8907-val_loss-0.8908.hdf5\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 8s 78ms/step - loss: 0.8904 - val_loss: 0.8905\n",
      "Epoch 52/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8901 - val_loss: 0.8900\n",
      "\n",
      "Epoch 00052: saving model to checkpoints_convolutional/siamese-epoch-00052-lr--train_loss-0.8901-val_loss-0.8900.hdf5\n",
      "Epoch 53/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8898 - val_loss: 0.8898\n",
      "Epoch 54/1000\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.8895 - val_loss: 0.8893\n",
      "\n",
      "Epoch 00054: saving model to checkpoints_convolutional/siamese-epoch-00054-lr--train_loss-0.8895-val_loss-0.8893.hdf5\n",
      "Epoch 55/1000\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.8893 - val_loss: 0.8893\n",
      "Epoch 56/1000\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.8890 - val_loss: 0.8884\n",
      "\n",
      "Epoch 00056: saving model to checkpoints_convolutional/siamese-epoch-00056-lr--train_loss-0.8890-val_loss-0.8884.hdf5\n",
      "Epoch 57/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8889 - val_loss: 0.8885\n",
      "Epoch 58/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8886 - val_loss: 0.8883\n",
      "\n",
      "Epoch 00058: saving model to checkpoints_convolutional/siamese-epoch-00058-lr--train_loss-0.8886-val_loss-0.8883.hdf5\n",
      "Epoch 59/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8884 - val_loss: 0.8883\n",
      "Epoch 60/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8882 - val_loss: 0.8882\n",
      "\n",
      "Epoch 00060: saving model to checkpoints_convolutional/siamese-epoch-00060-lr--train_loss-0.8882-val_loss-0.8882.hdf5\n",
      "Epoch 61/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8880 - val_loss: 0.8880\n",
      "Epoch 62/1000\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.8879 - val_loss: 0.8874\n",
      "\n",
      "Epoch 00062: saving model to checkpoints_convolutional/siamese-epoch-00062-lr--train_loss-0.8879-val_loss-0.8874.hdf5\n",
      "Epoch 63/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8877 - val_loss: 0.8874\n",
      "Epoch 64/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8875 - val_loss: 0.8873\n",
      "\n",
      "Epoch 00064: saving model to checkpoints_convolutional/siamese-epoch-00064-lr--train_loss-0.8875-val_loss-0.8873.hdf5\n",
      "Epoch 65/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8874 - val_loss: 0.8868\n",
      "Epoch 66/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8871 - val_loss: 0.8865\n",
      "\n",
      "Epoch 00066: saving model to checkpoints_convolutional/siamese-epoch-00066-lr--train_loss-0.8871-val_loss-0.8865.hdf5\n",
      "Epoch 67/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8869 - val_loss: 0.8863\n",
      "Epoch 68/1000\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.8867 - val_loss: 0.8869\n",
      "\n",
      "Epoch 00068: saving model to checkpoints_convolutional/siamese-epoch-00068-lr--train_loss-0.8867-val_loss-0.8869.hdf5\n",
      "Epoch 69/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8865 - val_loss: 0.8861\n",
      "Epoch 70/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8865 - val_loss: 0.8865\n",
      "\n",
      "Epoch 00070: saving model to checkpoints_convolutional/siamese-epoch-00070-lr--train_loss-0.8865-val_loss-0.8865.hdf5\n",
      "Epoch 71/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8864 - val_loss: 0.8851\n",
      "Epoch 72/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8859 - val_loss: 0.8858\n",
      "\n",
      "Epoch 00072: saving model to checkpoints_convolutional/siamese-epoch-00072-lr--train_loss-0.8859-val_loss-0.8858.hdf5\n",
      "Epoch 73/1000\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.8858 - val_loss: 0.8856\n",
      "Epoch 74/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8854 - val_loss: 0.8860\n",
      "\n",
      "Epoch 00074: saving model to checkpoints_convolutional/siamese-epoch-00074-lr--train_loss-0.8854-val_loss-0.8860.hdf5\n",
      "Epoch 75/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8852 - val_loss: 0.8850\n",
      "Epoch 76/1000\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.8849 - val_loss: 0.8842\n",
      "\n",
      "Epoch 00076: saving model to checkpoints_convolutional/siamese-epoch-00076-lr--train_loss-0.8849-val_loss-0.8842.hdf5\n",
      "Epoch 77/1000\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.8848 - val_loss: 0.8840\n",
      "Epoch 78/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8848 - val_loss: 0.8850\n",
      "\n",
      "Epoch 00078: saving model to checkpoints_convolutional/siamese-epoch-00078-lr--train_loss-0.8848-val_loss-0.8850.hdf5\n",
      "Epoch 79/1000\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.8846 - val_loss: 0.8859\n",
      "Epoch 80/1000\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.8845 - val_loss: 0.8832\n",
      "\n",
      "Epoch 00080: saving model to checkpoints_convolutional/siamese-epoch-00080-lr--train_loss-0.8845-val_loss-0.8832.hdf5\n",
      "Epoch 81/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8843 - val_loss: 0.8840\n",
      "Epoch 82/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8842 - val_loss: 0.8834\n",
      "\n",
      "Epoch 00082: saving model to checkpoints_convolutional/siamese-epoch-00082-lr--train_loss-0.8842-val_loss-0.8834.hdf5\n",
      "Epoch 83/1000\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.8842 - val_loss: 0.8847\n",
      "Epoch 84/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8843 - val_loss: 0.8839\n",
      "\n",
      "Epoch 00084: saving model to checkpoints_convolutional/siamese-epoch-00084-lr--train_loss-0.8843-val_loss-0.8839.hdf5\n",
      "Epoch 85/1000\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.8843 - val_loss: 0.8834\n",
      "Epoch 86/1000\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.8840 - val_loss: 0.8840\n",
      "\n",
      "Epoch 00086: saving model to checkpoints_convolutional/siamese-epoch-00086-lr--train_loss-0.8840-val_loss-0.8840.hdf5\n",
      "Epoch 87/1000\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.8840 - val_loss: 0.8848\n",
      "Epoch 88/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8840 - val_loss: 0.8848\n",
      "\n",
      "Epoch 00088: saving model to checkpoints_convolutional/siamese-epoch-00088-lr--train_loss-0.8840-val_loss-0.8848.hdf5\n",
      "Epoch 89/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8838 - val_loss: 0.8834\n",
      "Epoch 90/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8836 - val_loss: 0.8831\n",
      "\n",
      "Epoch 00090: saving model to checkpoints_convolutional/siamese-epoch-00090-lr--train_loss-0.8836-val_loss-0.8831.hdf5\n",
      "Epoch 91/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8837 - val_loss: 0.8835\n",
      "Epoch 92/1000\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.8833 - val_loss: 0.8831\n",
      "\n",
      "Epoch 00092: saving model to checkpoints_convolutional/siamese-epoch-00092-lr--train_loss-0.8833-val_loss-0.8831.hdf5\n",
      "Epoch 93/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8836 - val_loss: 0.8827\n",
      "Epoch 94/1000\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.8836 - val_loss: 0.8817\n",
      "\n",
      "Epoch 00094: saving model to checkpoints_convolutional/siamese-epoch-00094-lr--train_loss-0.8836-val_loss-0.8817.hdf5\n",
      "Epoch 95/1000\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.8834 - val_loss: 0.8834\n",
      "Epoch 96/1000\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.8834 - val_loss: 0.8834\n",
      "\n",
      "Epoch 00096: saving model to checkpoints_convolutional/siamese-epoch-00096-lr--train_loss-0.8834-val_loss-0.8834.hdf5\n",
      "Epoch 97/1000\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.8835 - val_loss: 0.8837\n",
      "Epoch 98/1000\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.8835 - val_loss: 0.8833\n",
      "\n",
      "Epoch 00098: saving model to checkpoints_convolutional/siamese-epoch-00098-lr--train_loss-0.8835-val_loss-0.8833.hdf5\n",
      "Epoch 99/1000\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 0.8835 - val_loss: 0.8829\n",
      "Epoch 100/1000\n",
      "100/100 [==============================] - 7s 65ms/step - loss: 0.8833 - val_loss: 0.8836\n",
      "\n",
      "Epoch 00100: saving model to checkpoints_convolutional/siamese-epoch-00100-lr--train_loss-0.8833-val_loss-0.8836.hdf5\n",
      "Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 7s 69ms/step - loss: 0.8834 - val_loss: 0.8825\n",
      "Epoch 102/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8834 - val_loss: 0.8842\n",
      "\n",
      "Epoch 00102: saving model to checkpoints_convolutional/siamese-epoch-00102-lr--train_loss-0.8834-val_loss-0.8842.hdf5\n",
      "Epoch 103/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8831 - val_loss: 0.8826\n",
      "Epoch 104/1000\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.8833 - val_loss: 0.8840\n",
      "\n",
      "Epoch 00104: saving model to checkpoints_convolutional/siamese-epoch-00104-lr--train_loss-0.8833-val_loss-0.8840.hdf5\n",
      "Epoch 105/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8832 - val_loss: 0.8839\n",
      "Epoch 106/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8833 - val_loss: 0.8806\n",
      "\n",
      "Epoch 00106: saving model to checkpoints_convolutional/siamese-epoch-00106-lr--train_loss-0.8833-val_loss-0.8806.hdf5\n",
      "Epoch 107/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8833 - val_loss: 0.8830\n",
      "Epoch 108/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8834 - val_loss: 0.8833\n",
      "\n",
      "Epoch 00108: saving model to checkpoints_convolutional/siamese-epoch-00108-lr--train_loss-0.8834-val_loss-0.8833.hdf5\n",
      "Epoch 109/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8832 - val_loss: 0.8830\n",
      "Epoch 110/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8834 - val_loss: 0.8818\n",
      "\n",
      "Epoch 00110: saving model to checkpoints_convolutional/siamese-epoch-00110-lr--train_loss-0.8834-val_loss-0.8818.hdf5\n",
      "Epoch 111/1000\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.8831 - val_loss: 0.8819\n",
      "Epoch 112/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8832 - val_loss: 0.8851\n",
      "\n",
      "Epoch 00112: saving model to checkpoints_convolutional/siamese-epoch-00112-lr--train_loss-0.8832-val_loss-0.8851.hdf5\n",
      "Epoch 113/1000\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.8831 - val_loss: 0.8840\n",
      "Epoch 114/1000\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.8831 - val_loss: 0.8818\n",
      "\n",
      "Epoch 00114: saving model to checkpoints_convolutional/siamese-epoch-00114-lr--train_loss-0.8831-val_loss-0.8818.hdf5\n",
      "Epoch 115/1000\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.8830 - val_loss: 0.8846\n",
      "Epoch 116/1000\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.8830 - val_loss: 0.8826\n",
      "\n",
      "Epoch 00116: saving model to checkpoints_convolutional/siamese-epoch-00116-lr--train_loss-0.8830-val_loss-0.8826.hdf5\n",
      "Epoch 117/1000\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.8830 - val_loss: 0.8822\n",
      "Epoch 118/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8831 - val_loss: 0.8836\n",
      "\n",
      "Epoch 00118: saving model to checkpoints_convolutional/siamese-epoch-00118-lr--train_loss-0.8831-val_loss-0.8836.hdf5\n",
      "Epoch 119/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8832 - val_loss: 0.8808\n",
      "Epoch 120/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8830 - val_loss: 0.8825\n",
      "\n",
      "Epoch 00120: saving model to checkpoints_convolutional/siamese-epoch-00120-lr--train_loss-0.8830-val_loss-0.8825.hdf5\n",
      "Epoch 121/1000\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.8831 - val_loss: 0.8820\n",
      "Epoch 122/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8830 - val_loss: 0.8840\n",
      "\n",
      "Epoch 00122: saving model to checkpoints_convolutional/siamese-epoch-00122-lr--train_loss-0.8830-val_loss-0.8840.hdf5\n",
      "Epoch 123/1000\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.8831 - val_loss: 0.8829\n",
      "Epoch 124/1000\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.8831 - val_loss: 0.8829\n",
      "\n",
      "Epoch 00124: saving model to checkpoints_convolutional/siamese-epoch-00124-lr--train_loss-0.8831-val_loss-0.8829.hdf5\n",
      "Epoch 125/1000\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.8829 - val_loss: 0.8827\n",
      "Epoch 126/1000\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.8831 - val_loss: 0.8823\n",
      "\n",
      "Epoch 00126: saving model to checkpoints_convolutional/siamese-epoch-00126-lr--train_loss-0.8831-val_loss-0.8823.hdf5\n",
      "Epoch 127/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8831 - val_loss: 0.8842\n",
      "Epoch 128/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8830 - val_loss: 0.8827\n",
      "\n",
      "Epoch 00128: saving model to checkpoints_convolutional/siamese-epoch-00128-lr--train_loss-0.8830-val_loss-0.8827.hdf5\n",
      "Epoch 129/1000\n",
      "100/100 [==============================] - 6s 62ms/step - loss: 0.8832 - val_loss: 0.8840\n",
      "Epoch 130/1000\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.8830 - val_loss: 0.8832\n",
      "\n",
      "Epoch 00130: saving model to checkpoints_convolutional/siamese-epoch-00130-lr--train_loss-0.8830-val_loss-0.8832.hdf5\n",
      "Epoch 131/1000\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.8832 - val_loss: 0.8842\n",
      "Epoch 132/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8831 - val_loss: 0.8828\n",
      "\n",
      "Epoch 00132: saving model to checkpoints_convolutional/siamese-epoch-00132-lr--train_loss-0.8831-val_loss-0.8828.hdf5\n",
      "Epoch 133/1000\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.8830 - val_loss: 0.8845\n",
      "Epoch 134/1000\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.8831 - val_loss: 0.8826\n",
      "\n",
      "Epoch 00134: saving model to checkpoints_convolutional/siamese-epoch-00134-lr--train_loss-0.8831-val_loss-0.8826.hdf5\n",
      "Epoch 135/1000\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.8830 - val_loss: 0.8836\n",
      "Epoch 136/1000\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.8831 - val_loss: 0.8817\n",
      "\n",
      "Epoch 00136: saving model to checkpoints_convolutional/siamese-epoch-00136-lr--train_loss-0.8831-val_loss-0.8817.hdf5\n",
      "Epoch 137/1000\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.8830 - val_loss: 0.8830\n",
      "Epoch 138/1000\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.8832 - val_loss: 0.8829\n",
      "\n",
      "Epoch 00138: saving model to checkpoints_convolutional/siamese-epoch-00138-lr--train_loss-0.8832-val_loss-0.8829.hdf5\n",
      "Epoch 139/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8829 - val_loss: 0.8824\n",
      "Epoch 140/1000\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.8829 - val_loss: 0.8822\n",
      "\n",
      "Epoch 00140: saving model to checkpoints_convolutional/siamese-epoch-00140-lr--train_loss-0.8829-val_loss-0.8822.hdf5\n",
      "Epoch 141/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8831 - val_loss: 0.8843\n",
      "Epoch 142/1000\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.8831 - val_loss: 0.8843\n",
      "\n",
      "Epoch 00142: saving model to checkpoints_convolutional/siamese-epoch-00142-lr--train_loss-0.8831-val_loss-0.8843.hdf5\n",
      "Epoch 143/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8831 - val_loss: 0.8824\n",
      "Epoch 144/1000\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.8831 - val_loss: 0.8821\n",
      "\n",
      "Epoch 00144: saving model to checkpoints_convolutional/siamese-epoch-00144-lr--train_loss-0.8831-val_loss-0.8821.hdf5\n",
      "Epoch 145/1000\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.8828 - val_loss: 0.8823\n",
      "Epoch 146/1000\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.8830 - val_loss: 0.8821\n",
      "\n",
      "Epoch 00146: saving model to checkpoints_convolutional/siamese-epoch-00146-lr--train_loss-0.8830-val_loss-0.8821.hdf5\n",
      "Epoch 147/1000\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 0.8828 - val_loss: 0.8835\n",
      "Epoch 148/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8831 - val_loss: 0.8815\n",
      "\n",
      "Epoch 00148: saving model to checkpoints_convolutional/siamese-epoch-00148-lr--train_loss-0.8831-val_loss-0.8815.hdf5\n",
      "Epoch 149/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8829 - val_loss: 0.8834\n",
      "Epoch 150/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8832 - val_loss: 0.8826\n",
      "\n",
      "Epoch 00150: saving model to checkpoints_convolutional/siamese-epoch-00150-lr--train_loss-0.8832-val_loss-0.8826.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8830 - val_loss: 0.8825\n",
      "Epoch 152/1000\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.8830 - val_loss: 0.8836\n",
      "\n",
      "Epoch 00152: saving model to checkpoints_convolutional/siamese-epoch-00152-lr--train_loss-0.8830-val_loss-0.8836.hdf5\n",
      "Epoch 153/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8830 - val_loss: 0.8836\n",
      "Epoch 154/1000\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.8830 - val_loss: 0.8831\n",
      "\n",
      "Epoch 00154: saving model to checkpoints_convolutional/siamese-epoch-00154-lr--train_loss-0.8830-val_loss-0.8831.hdf5\n",
      "Epoch 155/1000\n",
      " 62/100 [=================>............] - ETA: 2s - loss: 0.8830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-b3ce0d8798e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     use_multiprocessing=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=generate(32), \n",
    "                    steps_per_epoch=100, \n",
    "                    epochs=1000,\n",
    "                    initial_epoch=initial_epoch,\n",
    "                    validation_data=generate(32, 'val'), \n",
    "                    validation_steps=2,\n",
    "                    callbacks=[checkpoint, tensorboard, lr_decay],\n",
    "                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = get_batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49843925],\n",
       "       [0.4967049 ],\n",
       "       [0.5007879 ],\n",
       "       [0.4975088 ]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'lambda_1'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'model_1'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).get_output_at(-1))\n",
    "intermediate_output = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs,targets) = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "        val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "        model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour_correct(pairs,targets):\n",
    "    \"\"\"returns 1 if nearest neighbour gets the correct answer for a one-shot task\n",
    "        given by (pairs, targets)\"\"\"\n",
    "    L2_distances = np.zeros_like(targets)\n",
    "    for i in range(len(targets)):\n",
    "        L2_distances[i] = np.sum(np.sqrt(pairs[0][i]**2 - pairs[1][i]**2))\n",
    "    if np.argmin(L2_distances) == np.argmax(targets):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn_accuracy(N_ways,n_trials):\n",
    "    \"\"\"Returns accuracy of NN approach \"\"\"\n",
    "    print(\"Evaluating nearest neighbour on {} unique {} way one-shot learning tasks ...\".format(n_trials,N_ways))\n",
    "\n",
    "    n_right = 0\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        pairs,targets = make_oneshot_task(N_ways,\"val\")\n",
    "        correct = nearest_neighbour_correct(pairs,targets)\n",
    "        n_right += correct\n",
    "    return 100.0 * n_right / n_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N,s)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways = np.arange(1,20,2)\n",
    "resume =  False\n",
    "trials = 50\n",
    "val_accs, train_accs,nn_accs = [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 50 random 1 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 1 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 1 way one-shot learning tasks ...\n",
      "NN Accuracy =  100.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 3 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.0% 3 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 3 way one-shot learning tasks ...\n",
      "NN Accuracy =  64.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 5 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 5 way one-shot learning tasks ...\n",
      "NN Accuracy =  42.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 7 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 7 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 7 way one-shot learning tasks ...\n",
      "NN Accuracy =  48.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 9 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 20.0% 9 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 9 way one-shot learning tasks ...\n",
      "NN Accuracy =  30.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 11 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.0% 11 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 11 way one-shot learning tasks ...\n",
      "NN Accuracy =  36.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 13 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 22.0% 13 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 13 way one-shot learning tasks ...\n",
      "NN Accuracy =  26.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 15 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 22.0% 15 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 15 way one-shot learning tasks ...\n",
      "NN Accuracy =  32.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 17 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 16.0% 17 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 17 way one-shot learning tasks ...\n",
      "NN Accuracy =  28.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Evaluating model on 50 random 19 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 16.0% 19 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 50 unique 19 way one-shot learning tasks ...\n",
      "NN Accuracy =  32.0\n",
      "---------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for N in ways:    \n",
    "    val_accs.append(test_oneshot(model, N, trials, \"val\", verbose=True))\n",
    "#     train_accs.append(test_oneshot(model, N, trials, \"train\", verbose=True))\n",
    "    nn_acc = test_nn_accuracy(N, trials)\n",
    "    nn_accs.append(nn_acc)\n",
    "    print (\"NN Accuracy = \", nn_acc)\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path,\"accuracies.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((val_accs,train_accs,nn_accs),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_images(X):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc, h , w, _ = X.shape\n",
    "    X = X.reshape(nc, h, w)\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros((n*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for example in range(nc):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = X[example]\n",
    "        y += 1\n",
    "        if y >= n:\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oneshot_task(pairs):\n",
    "    fig,(ax1,ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    ax1.matshow(pairs[0][0].reshape(105,105), cmap='gray')\n",
    "    img = concat_images(pairs[1])\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of concat image visualization\n",
    "pairs, targets = make_oneshot_task(16,\"train\",\"Sanskrit\")\n",
    "plot_oneshot_task(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "ax.plot(ways, val_accs, \"m\", label=\"Siamese(val set)\")\n",
    "ax.plot(ways, train_accs, \"y\", label=\"Siamese(train set)\")\n",
    "plt.plot(ways, nn_accs, label=\"Nearest neighbour\")\n",
    "\n",
    "ax.plot(ways, 100.0/ways, \"g\", label=\"Random guessing\")\n",
    "plt.xlabel(\"Number of possible classes in one-shot tasks\")\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.title(\"Omiglot One-Shot Learning Performance of a Siamese Network\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "inputs,targets = make_oneshot_task(20, \"val\", 'Oriya')\n",
    "plt.show()\n",
    "\n",
    "plot_oneshot_task(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
